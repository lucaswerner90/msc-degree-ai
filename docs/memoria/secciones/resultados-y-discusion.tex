% RESULTADOS Y DISCUSION 

\cleardoublepage

\chapter{Resultados y Discusión}
\label{resultados-y-discusion}

\section{Introducción}
\label{resultados-introduccion}

A continuación haremos un repaso de los resultados obtenidos en el subconjunto de experimentos más representativo, realizados con los diferentes algoritmos propuestos.
\medskip

Detallaremos brevemente también las conclusiones parciales que obtenemos en cada experimento, así como posibles soluciones a los problemas que se nos plantean en cada uno.
\medskip

Cabe mencionar que debido al coste computacional de ejecutar cada experimento sobre el conjunto de datos total, cada experimento se compone de subexperimentos previos, en los cuales intentábamos probar nuestras soluciones con un conjunto de datos menor al original, alrededor de unas 20 imágenes. 
\medskip

La idea detrás de esta subexperimentación era conseguir un modelo de agente que sea capaz de realizar \textit{overfitting} sobre el conjunto de datos de manera relativamente rápida y que obtuviese una convergencia tanto en la recompensa obtenida tanto en las imágenes del conjunto de entrenamiento como en el conjunto de test como en el número de pasos que realizaba sobre cada una de las imágenes. Esto nos permitió también acortar el tiempo entre las diferentes pruebas, ya que el conseguir este overfitting nos aseguraba que el agente seguiría en un principio las ideas que queríamos implementar y además provocó que pudiesemos experimentar con diferentes definiciones tanto de nuestra recompensa como de nuestro entorno, así como hacer \textit{fine tunning} de nuestros hiperparámetros.
\medskip

Durante los experimentos podremos comprobar que en alguno de ellos el número de métricas que mostraremos es más alto que en otros, esto no es solo debido a que queramos destacar una cualidad específica del experimento, sino que tambien formó parte de la experimentación y el descubrimiento de algunos de los problemas que nos fuimos encontrando, que nos llevó a tener que realizar el seguimiento de algunos aspectos del entrenamiento que no estaban contemplados en experimentos anteriores. Estos problemas estuvieron relacionados principalmente con la convergencia en la toma de decisiones del agente tal y como se discutirá más adelante.
\medskip

Por último también mostraremos algunos resultados de las imágenes que fuimos obteniendo en cada caso. Cabe recordar que la ejecución del agente durante el test se detiene cuando decide ejecutar la acción de permanecer quieto, y que tanto esta recompensa como el número de acciones hasta llegar a ella se tienen en cuenta como medidas subjetivas de la eficacia del agente, tal y como comentamos en la sección \ref{evaluacion}.
\medskip

El total de los experimentos está disponible en el repositorio de \href{https://github.com/lucaswerner90/msc-degree-ai}{GitHub} para un análisis más extenso y se adjunta el título de cada uno de ellos en el Apéndize \ref{apendize-a}. 
\medskip

La herramienta utilizada para monitorear el progreso de las diferentes métricas durante los diferentes entrenamientos es \href{https://www.tensorflow.org/tensorboard?hl=es-419}{Tensorboard}, ya que además de su facil integración con \href{https://pytorch.org/}{PyTorch} nos permitía comparar dichas métricas entre los diferentes experimentos.
\medskip

\section{Policy Gradient}
\label{resultados-policy-gradient}

El algoritmo \textit{Policy Gradient} fue nuestra primera opción a la hora de implementar nuestro agente. Se buscaba obtener una primera aproximación a nuestro problema y buscábamos sobre todo tener un modelo que sirviese como \textit{baseline}, así como definir la estructura general de nuestro programa.
\medskip

\subsection{Experimento 1}
\label{resultados-policy-gradient-experimento-1}

En este primer experimento utilizamos como criterio de finalización del episodio que el agente realizase un máximo de 50 acciones o que obtuviese la máxima recompensa del entorno, en este caso se mantuvo la recompensa proporcional con la distancia al punto objetivo y no se hizo ninguna recompensa extra por llegar al final del episodio correctamente.
\medskip

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/policy_gradient/policy_gradient_normalized_image_reward_20_epochs/training_reward_mean.png}
	\caption[Experimento Policy Gradient 1 - Training Reward Mean]{Experimento Policy Gradient 1 - Training Reward Mean}
	\label{fig-experimento-policy-gradient-1-training-reward-mean}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/policy_gradient/policy_gradient_normalized_image_reward_20_epochs/testing_reward_mean.png}
	\caption[Experimento Policy Gradient 1 - Testing reward mean]{Experimento Policy Gradient 1 - Testing reward mean}
	\label{fig-experimento-policy-gradient-1-testing-reward-mean}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/policy_gradient/policy_gradient_normalized_image_reward_20_epochs/episodes_duration.png}
	\caption[Experimento Policy Gradient 1 - Episodes duration]{Experimento Policy Gradient 1 - Episodes duration}
	\label{fig-experimento-policy-gradient-1-episodes-duration}
\end{figure}

Lo que se puede apreciar es que existe una tendencia a la baja de nuestro algoritmo en cuanto al número de acciones en el conjunto de entrenamiento. Por otra parte, se aprecia que la recompensa en el conjunto de test comienza a subir a partir de la época 10, lo cual marca un punto importante de inflexión en esta. 

\subsection{Experimento 2}
\label{resultados-policy-gradient-experimento-2}

Los parámetros utilizados en este experimento son similares al anterior, pero en este caso el entrenamiento se ejecuta durante unas 50 épocas, unas 30 más que el experimento \ref{resultados-policy-gradient-experimento-1}.
\medskip

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/policy_gradient/policy_gradient_normalized_image_reward_50_epochs/training_reward_mean.png}
	\caption[Experimento Policy Gradient 2 - Training Reward Mean]{Experimento Policy Gradient 2 - Training Reward Mean}
	\label{fig-experimento-policy-gradient-2-training-reward-mean}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/policy_gradient/policy_gradient_normalized_image_reward_50_epochs/testing_reward_mean.png}
	\caption[Experimento Policy Gradient 2 - Testing reward mean]{Experimento Policy Gradient 2 - Testing reward mean}
	\label{fig-experimento-policy-gradient-2-testing-reward-mean}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/policy_gradient/policy_gradient_normalized_image_reward_50_epochs/episodes_duration.png}
	\caption[Experimento Policy Gradient 2 - Episodes duration]{Experimento Policy Gradient 2 - Episodes duration}
	\label{fig-experimento-policy-gradient-2-episodes-duration}
\end{figure}

Vemos que en este experimento la tendencia que vimos en el experimento anterior se confirma: el número de acciones por imagen en el conjunto de entrenamiento parece seguir una tendencia a la baja y tanto la recompensa en el conjunto de test como en el entrenamiento aumentan según van avanzando las épocas.
\medskip

\section{Actor-Critic}
\label{resultados-actor-critic}

Veremos ahora el comportamiento de algunos de los experimentos llevados a cabo con el algoritmo Actor Critic.
\medskip

\subsection{Experimento 1}
\label{resultados-actor-critic-experimento-1}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/actor_critic/actor_critic_20_epochs/train_rewards.png}
	\caption[Experimento Actor Critic 1 - Training Reward Mean]{Experimento Actor Critic 1 - Training Reward Mean}
	\label{fig-experimento-actor-critic-1-training-reward-mean}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/actor_critic/actor_critic_20_epochs/test_mean_reward.png}
	\caption[Experimento Actor Critic 1 - Testing reward mean]{Experimento Actor Critic 1 - Testing reward mean}
	\label{fig-experimento-actor-critic-1-testing-reward-mean}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/actor_critic/actor_critic_20_epochs/episodes_duration.png}
	\caption[Experimento Actor Critic 1 - Episodes duration]{Experimento Actor Critic 1 - Episodes duration}
	\label{fig-experimento-actor-critic-1-episodes-duration}
\end{figure}

\subsection{Experimento 2}
\label{resultados-actor-critic-experimento-2}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/actor_critic/actor_critic_no_rewards_till_complete/train_rewards.png}
	\caption[Experimento Actor Critic 2 - Training Reward Mean]{Experimento Actor Critic 2 - Training Reward Mean}
	\label{fig-experimento-actor-critic-2-training-reward-mean}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/actor_critic/actor_critic_no_rewards_till_complete/test_mean_reward.png}
	\caption[Experimento Actor Critic 2 - Testing reward mean]{Experimento Actor Critic 2 - Testing reward mean}
	\label{fig-experimento-actor-critic-2-testing-reward-mean}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/actor_critic/actor_critic_no_rewards_till_complete/episodes_duration.png}
	\caption[Experimento Actor Critic 2 - Episodes duration]{Experimento Actor Critic 2 - Episodes duration}
	\label{fig-experimento-actor-critic-2-episodes-duration}
\end{figure}

\subsection{Experimento 3}
\label{resultados-actor-critic-experimento-3}


\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/policy_gradient/policy_gradient_normalized_image_reward_50_epochs/training_reward_mean.png}
	\caption[Experimento Actor Critic 3 - Training Reward Mean]{Experimento Actor Critic 3 - Training Reward Mean}
	\label{fig-experimento-actor-critic-3-training-reward-mean}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/policy_gradient/policy_gradient_normalized_image_reward_50_epochs/testing_reward_mean.png}
	\caption[Experimento Actor Critic 3 - Testing reward mean]{Experimento Actor Critic 3 - Testing reward mean}
	\label{fig-experimento-actor-critic-3-testing-reward-mean}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/policy_gradient/policy_gradient_normalized_image_reward_50_epochs/episodes_duration.png}
	\caption[Experimento Actor Critic 3 - Episodes duration]{Experimento Actor Critic 3 - Episodes duration}
	\label{fig-experimento-actor-critic-3-episodes-duration}
\end{figure}

\section{Vision Transformers}
\label{resultados-vision-transformers}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/vision transformers/train_rewards.png}
	\caption[Experimento \textit{Vision Transformer} - Training Reward Mean]{Experimento \textit{Vision Transformer} - Training Reward Mean}
	\label{fig-experimento-vision-transformer-1-training-reward-mean}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/vision transformers/train_loss.png}
	\caption[Experimento \textit{Vision Transformer} - Training Loss]{Experimento \textit{Vision Transformer} - Training Loss}
	\label{fig-experimento-vision-transformer-1-training-loss}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/vision transformers/test_mean_reward.png}
	\caption[Experimento \textit{Vision Transformer} - Testing reward mean]{Experimento \textit{Vision Transformer} - Testing reward mean}
	\label{fig-experimento-vision-transformer-1-testing-reward-mean}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figuras/experiments/vision transformers/episodes_duration.png}
	\caption[Experimento \textit{Vision Transformer} - Episodes duration]{Experimento \textit{Vision Transformer} - Episodes duration}
	\label{fig-experimento-vision-transformer-1-episodes-duration}
\end{figure}